{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68717f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Business Problem :  Develop a project that utilizes LangChain for question-answering \n",
    "based on four provided files\n",
    "1.1. Business Objective:\n",
    "The objective is to develop a question-answering system that can interact with PDF\n",
    "and Word documents using OpenAI's API, providing detailed answers based on the content\n",
    "of these documents. This tool is designed to help users quickly extract relevant \n",
    "information from large documents by asking specific questions.\n",
    "\n",
    "1.2. Constraints:\n",
    "> The system relies on the OpenAI API, which has a usage limit and cost associated with it.\n",
    "> The quality of the answers depends on the context available in the documents and the \n",
    "efficiency of the text chunking and embedding processes.\n",
    "> Users must upload valid PDF or Word documents, and the system's performance may vary \n",
    "based on document size and content complexity.\n",
    "\n",
    "2. Work on Each Feature of the Dataset\n",
    "2.1. Data Dictionary:\n",
    "For this project, the dataset consists of text extracted from PDF and Word documents. \n",
    "While a traditional data dictionary is not applicable lets see an table.\n",
    "\n",
    "3. Data Pre-processing\n",
    "3.1. Data Cleaning, Feature Engineering, etc.:\n",
    "\n",
    "Text Extraction: Text is extracted from PDF and Word documents.\n",
    "Feature Engineering: Text chunks are created to ensure the context is maintained\n",
    "and to handle large documents efficiently.\n",
    "\n",
    "3.2. Outlier Treatment:\n",
    "Thereâ€™s no specific outlier treatment as the project deals with text data,\n",
    "but text chunks are managed to maintain consistency and relevance.\n",
    "\n",
    "4. Exploratory Data Analysis (EDA)\n",
    "4.1. Summary:\n",
    "The project involves extracting, chunking, and embedding text from documents for question-answering.\n",
    "4.2. Univariate Analysis:\n",
    "4.3. Bivariate Analysis:\n",
    "\n",
    "5.\tModel Building\n",
    "The project involves creating a system that extracts text from PDF and Word documents, \n",
    "breaks it into manageable chunks, and converts these chunks into vector embeddings using\n",
    "OpenAI's model. These embeddings are stored in a FAISS index, enabling efficient similarity\n",
    "searches to retrieve relevant text based on user queries. A question-answering chain then\n",
    "generates detailed answers using the retrieved text. Users interact with this system\n",
    "through a Streamlit interface, allowing them to ask questions and receive precise answers\n",
    "from the uploaded documents. The model's effectiveness hinges on the seamless integration \n",
    "of text processing, embedding, and retrieval components.\n",
    "\n",
    "6. Benefits/Impact of the Solution\n",
    "The business benefits from this solution by providing users with a powerful tool to \n",
    "extract relevant information from large and complex documents efficiently.\n",
    "This saves time and enhances decision-making processes, especially in environments \n",
    "where quick access to specific information is crucial.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c4e2136",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from PyPDF2 import PdfReader\n",
    "from docx import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5485257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "# Environment Variable to Acess OPENAI API KEY\n",
    "# good practise not to hard code API keys\n",
    "# load environment variables from a file named .env into the environment.\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09ba4056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from PDF files\n",
    "# creating a corpus form the pdf files.\n",
    "# The open function is used to open the PDF file located at path.\n",
    "# The \"rb\" mode specifies that the file is opened in \"read-binary\" mode, which is necessary for handling PDF files.\n",
    "def pdf_text(pdf_paths):\n",
    "    text = \"\"\n",
    "    for path in pdf_paths:\n",
    "        with open(path, \"rb\") as file:\n",
    "            pdf_reader = PdfReader(file)\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Function to extract text from DOCX files\n",
    "# creating a corpus form the word files\n",
    "def docx_text(docx_paths):\n",
    "    text = \"\"\n",
    "    for path in docx_paths:\n",
    "        doc_reader = Document(path)\n",
    "        for paragraph in doc_reader.paragraphs:\n",
    "            text += paragraph.text + \"\\n\"\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fe01aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into chunks\n",
    "# split a long text into smaller chunks\n",
    "# recursively tries to split the text into chunks that are under a specified size\n",
    "# Insuring that important content that spans the boundary of two chunks is included in both.\n",
    "def get_text_chunks(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
    "    return text_splitter.split_text(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46b20489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and save vector store\n",
    "# FAISS : Facebook AI similarity search\n",
    "# gets vector from the extracted text after making chunks\n",
    "def get_vector_store(text_chunks):\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\", openai_api_key=api_key)\n",
    "    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)\n",
    "    vector_store.save_local(\"faiss_index\")\n",
    "# The FAISS.from_texts method converts the text_chunks into vectors using the embeddings\n",
    "# model and creates a FAISS index to store these vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d28eaf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the QA chain\n",
    "# providing the prompts\n",
    "\n",
    "def get_conversational_chain():\n",
    "    # template for generating responses. \n",
    "    prompt_template = \"\"\"\n",
    "    Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is not in\n",
    "    provided context just say, \"answer is not available in the context\", don't provide the wrong answer\\n\\n\n",
    "    Context:\\n {context}?\\n\n",
    "    Question: \\n{question}\\n\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    model = OpenAI(temperature=0.0, openai_api_key=api_key) # low temperature (0.0) for more deterministic responses\n",
    "    # Creates a PromptTemplate object using the predefined template. The template expects two input variables: context and question.\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "    return load_qa_chain(model, chain_type=\"stuff\", prompt=prompt)#Loads a question-answering (QA) chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66117f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle user input and generate answers\n",
    "def user_input(user_question):\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\", openai_api_key=api_key)\n",
    "    new_db = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "    # Loads a FAISS vector store from a local file named faiss_index.\n",
    "    docs = new_db.similarity_search(user_question)\n",
    "    # Performs a similarity search in the FAISS index using the user's question to find relevant documents or text chunks.\n",
    "    chain = get_conversational_chain()\n",
    "    # Calls the get_conversational_chain function to retrieve the QA chain for generating responses.\n",
    "    response = chain({\"input_documents\": docs, \"question\": user_question}, return_only_outputs=True)\n",
    "    print(\"Reply:\", response[\"output_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06b22b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Complete\n",
      "Ask a Question from the Uploaded Files (or type 'exit' to quit): How Many Indian States\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91721\\anaconda3\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n",
      "C:\\Users\\91721\\anaconda3\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: This class is deprecated. See the following migration guides for replacements based on `chain_type`:\n",
      "stuff: https://python.langchain.com/v0.2/docs/versions/migrating_chains/stuff_docs_chain\n",
      "map_reduce: https://python.langchain.com/v0.2/docs/versions/migrating_chains/map_reduce_chain\n",
      "refine: https://python.langchain.com/v0.2/docs/versions/migrating_chains/refine_chain\n",
      "map_rerank: https://python.langchain.com/v0.2/docs/versions/migrating_chains/map_rerank_docs_chain\n",
      "\n",
      "See also guides on retrieval and question-answering here: https://python.langchain.com/v0.2/docs/how_to/#qa-with-rag\n",
      "  warn_deprecated(\n",
      "C:\\Users\\91721\\anaconda3\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reply: \n",
      "India is a federal union comprising 28 states and 8 union territories.\n",
      "Ask a Question from the Uploaded Files (or type 'exit' to quit): what support is given by ayushman bharat scheme\n",
      "Reply: \n",
      "The Ayushman Bharat scheme aims to provide health insurance coverage to millions of poor and vulnerable people in India. This support is crucial for improving healthcare access and affordability, especially in rural areas.\n",
      "Ask a Question from the Uploaded Files (or type 'exit' to quit): what is the population of India\n",
      "Reply: \n",
      "India has a population of over 1.3 billion, making it the world's second most populous country.\n",
      "Ask a Question from the Uploaded Files (or type 'exit' to quit): what is the highest mountain range in India\n",
      "Reply: The highest mountain range in India is the Himalayas.\n",
      "Ask a Question from the Uploaded Files (or type 'exit' to quit): exit\n"
     ]
    }
   ],
   "source": [
    "# Main function\n",
    "def main():\n",
    "   # Embedded file paths\n",
    "    pdf_paths = [\n",
    "       \"C:/Assignment/Training Data/India A Comprehensive Overview.pdf\",\n",
    "        \"C:/Assignment/Training Data/India's Diverse States and Territories.pdf\"\n",
    "    ]\n",
    "    docx_paths = [\n",
    "       \"C:/Assignment/Training Data/India's Education, Healthcare, and Social Development.docx\",\n",
    "        \"C:/Assignment/Training Data/India's Natural Beauty and Wildlife.docx\"\n",
    "    ]\n",
    "    \n",
    "    raw_text = \"\"\n",
    "    # for the pdf_paths and docx_paths we call the get_pdf_text and get_doc_text fuctions and store it\n",
    "    # in raw_text , the raw text contains the corpus of the file text data\n",
    "    if pdf_paths:\n",
    "        raw_text += pdf_text(pdf_paths)\n",
    "    if docx_paths:\n",
    "        raw_text += docx_text(docx_paths)\n",
    "\n",
    "    # split a long text into smaller chunks using the get_text_chunks function \n",
    "    # works like tokenization , where tokens are created\n",
    "    # get_vector_store function takes the tokenized output and create the vectors to be \n",
    "    # given to the model\n",
    "    if raw_text:\n",
    "        text_chunks = get_text_chunks(raw_text)\n",
    "        get_vector_store(text_chunks)\n",
    "        print(\"Processing Complete\")\n",
    "\n",
    "        while True:\n",
    "            user_question = input(\"Ask a Question from the Uploaded Files (or type 'exit' to quit): \")\n",
    "            if user_question.lower() == 'exit':\n",
    "                break\n",
    "            user_input(user_question)\n",
    "    else:\n",
    "        print(\"No valid files uploaded\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11106c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaa6468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88424464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
